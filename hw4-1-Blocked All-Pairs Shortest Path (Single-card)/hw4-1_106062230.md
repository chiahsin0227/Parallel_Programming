# Parallel Programming hw4-1

###### tags: `PP20`

106062230 徐嘉欣

## Implementation

首先，我在做input時，有做padding的部分，讓整個2D array的長寬都是64的倍數(因為**Blocking factor取64**)，這樣在device端就不用怕存取到超過memory範圍的部分。

```cpp
fread(&v, sizeof(int), 1, file);
fread(&m, sizeof(int), 1, file);
if(v%64) n = v + (64 - v%64);
else n = v;
cudaMallocHost( &Dist, sizeof(int)*(n*n)); 
```

而在做blocked floyd warshall時，因為device的每個block最多只能包含1024個threads，因此這邊取的**thread是dim3(32, 32)**，比起每次都只有做一遍的computing，一次做4遍的computing(有點像在模擬(64,64)的thread)、盡可能最大地利用shared memory的大小會使效能變得好些。

![](https://i.imgur.com/lJO6H2o.png)

而每個2D array切割成的各個block剛好會分給device中的不同block，因此device的block數取得則是各個phase中，長與寬分別有多少block(i.e., **dim3(block_width, block_height)**)。

```cpp
dim3 thread(32, 32);

/* Phase 1*/
phase1<<< 1, thread, B*B*sizeof(int) >>>(B, r,    r,  r, n, d_dist, pitch_int);

/* Phase 2*/
phase2_2<<<dim3(r, 1),             thread, 8192*sizeof(int)>>>(B, r, n, d_dist, pitch_int, r, 0);  // up
phase2_2<<<dim3(round - r - 1, 1), thread, 8192*sizeof(int)>>>(B, r, n, d_dist, pitch_int, r, r+1);  // down
phase2_1<<<dim3(1, r),             thread, 8192*sizeof(int)>>>(B, r, n, d_dist, pitch_int, 0, r); // left
phase2_1<<<dim3(1, round - r - 1), thread, 8192*sizeof(int)>>>(B, r, n, d_dist, pitch_int, r+1, r); // right

/* Phase 3*/
phase3<<<dim3(r, r), thread, 8192*sizeof(int)>>>(B, k_min, n, d_dist, pitch_int, 0, 0);
phase3<<<dim3(round - r - 1, r), thread, 8192*sizeof(int)>>>(B, k_min, n, d_dist, pitch_int, 0, r+1);
phase3<<<dim3(r, round - r - 1), thread, 8192*sizeof(int)>>>(B, k_min, n, d_dist, pitch_int, r+1, 0);
phase3<<<dim3(round - r - 1, round - r - 1), thread, 8192*sizeof(int)>>>(B, k_min, n, d_dist, pitch_int, r+1, r+1);
```

phase1與phase2, phase3大部分蠻相近的，只有一小部分的念頭不同，因此先解釋phase1，phase2與3只挑選與phase1不同的部分做解釋。

會將phase1~phase3改寫成各個不同的function，是因為不同的phase中，`Dist[i][j]`、`Dist[i][k]`、`Dist[k][j]`有可能會有重疊在同一塊block的情況，如phase1中，`Dist[i][j]`、`Dist[i][k]`、`Dist[k][j]`都會落在同一個block中，因此只需要load global memory至一個shared memory中就好了。

首先先計算出`Dist[i][j]`、`Dist[i][k]`、`Dist[k][j]`所在的block的左上方的頂點，再來對於`dist[i][j]`load 4遍的global memory，因為thread的大小最大是1024，因此不能開(64, 64)的thread，因此用(32, 32)的話，要再多讀3個global memory。讀完之後就可以進行floyd warshall了，在判斷是否有更小的路徑時，原先使用if去做判斷，但**這樣可能會有diversity**的狀況，致使效能降低，因此改用`min`去取較小的值，讓warp中的所有人都做一樣的事，降低diversity。最後再存回global memory就完成phase1了。

```cpp
__global__ void phase1(int B, int Round, int block_start_x, int block_start_y, int n, int* d_dist, int p) {

    int b_i = (block_start_x << 6) + threadIdx.y;
    int b_j = (block_start_y << 6) + threadIdx.x;
    
    extern __shared__ int shared_mem[]; 
    
    #pragma unroll
    for(int r=0; r<2; ++r){
        int idx = threadIdx.y + (r << 5);
        shared_mem[idx*B + threadIdx.x] = d_dist[(b_i + (r << 5))*p + b_j]; 
        shared_mem[idx*B + threadIdx.x + 32] = d_dist[(b_i + (r << 5))*p + b_j + 32];      
    }

    #pragma unroll
    for (int k = 0; k < 64; ++k) {
        __syncthreads();
        for(int r=0; r<2; ++r){       
            int idx = threadIdx.y + (r << 5);
            shared_mem[idx*B+threadIdx.x] = min(shared_mem[idx*B+threadIdx.x], shared_mem[idx*B+k] + shared_mem[k*B+threadIdx.x]);
            shared_mem[idx*B+threadIdx.x + 32] = min(shared_mem[idx*B+threadIdx.x + 32], shared_mem[idx*B+k] + shared_mem[k*B+threadIdx.x + 32]);
        }
    }

    #pragma unroll
    for(int r=0; r<2; ++r){
        d_dist[(b_i + (r << 5))*p + b_j] = shared_mem[(threadIdx.y + (r << 5))*B + threadIdx.x];  
        d_dist[(b_i + (r << 5))*p + b_j + 32] = shared_mem[(threadIdx.y + (r << 5))*B + threadIdx.x + 32];  
    }
}
```

phase2則分成`phase2_1`與`phase2_2`兩個:

* `phase2_1`是負責pivot左右的長條狀block們，因為負責的是pivot左右兩塊，因此`Dist[i][j]`與`Dist[i][k]`其實是落在同一個block中，因此這兩個可以共同儲存在同一個shared memory中。
* `phase2_2`是負責pivot上下的長條狀block們，因為負責的是pivot上下兩塊，因此`Dist[i][j]`與`Dist[k][j]`其實是落在同一個block中，因此這兩個可以共同儲存在同一個shared memory中。

phase3的話，因為`Dist[i][j]`、`Dist[i][k]`、`Dist[k][j]`都沒有重疊到，再加上其實只有自己會用到`Dist[i][j]`(前面的phase因為`Dist[i][j]`皆有與其他Dist重複到，因此要load進shared memory做共用)，所以不需要再開shared memory給`Dist[i][j]`儲存，只需儲存`Dist[i][k]`、`Dist[k][j]`就好，`Dist[i][j]`可以使用register去記錄。

## Profiling Results

The results below are based on running testcase c21.1.

![](https://i.imgur.com/sJIdjBv.png)

## Experiment & Analysis

### System Spec

使用hades來做實驗與測量。

### Time Distribution

computing time與memory copy time(H2D, D2H)都是透過`nvprof`來測量，其中computing time為phase1, phase2-1, phase2-2, 與phase3四者的時間總和。而I/O time則是透過以下的方式測量：

```cpp
std::chrono::steady_clock::time_point t1 = std::chrono::steady_clock::now();
/* doing I/O here */
std::chrono::steady_clock::time_point t2 = std::chrono::steady_clock::now();
std::cout << "Reading(or writing) file took " << std::chrono::duration_cast<std::chrono::microseconds>(t2 - t1).count() << "us.\n";
```

| testcase |  n  |    m    |   input (second)  | output (second)| computing (second)|   H2D (second)  |   D2H (second) |
|:--------:|:---:|:-------:|:---------:|:------:|:---------:|:-------:|:-------:|
|   p25k1  |25000|5780158  |0.0323     |0.9038  |17.1074    |0.4167   |0.3790   |
|   p20k1  |20000|264275   |0.0029     |0.4353  |9.0497     |0.2473   |0.2434   |
|   p15k1  |15000|5591272  |0.0342     |0.3328  |3.7257     |0.1509   |0.1370   |
|   p11k1  |11000|505586   |0.0048     |0.1815  |2.0307     |0.0808   |0.0733   |
|   c21.1  |5000 |10723117 |0.0627     |0.0407  |0.1594     |0.0170   |0.0154   |

![](https://i.imgur.com/y6lnxpP.png)

可發現做input的時間與m為正相關(因為要讀取m這麼多個距離的pair進來)，而output、computing、memory copy則都是與n為正相關。

### Blocking Factor

The results below is based on running testcase c21.1.

global memory(shared memory memory) bandwidth的測量方式是使用`nvprof`加上`-m gld_throughput,gst_throughput`(`-m shared_load_throughtput,shared_store_throughput`)獲得，這邊取phase3的average來做比較，獲取方式如下圖:

![](https://i.imgur.com/N4O6gGB.png)

而測量GOPS的方式則是先使用`nvprof`加上`-m inst_integer`，拿phase3的average乘以phase3總共跑的次數，以獲得phase3總共的integer instructions次數，再使用`nvprof`去獲取執行phase3的時間去除，就能獲得GOPS。

![](https://i.imgur.com/w9AIx8f.png)

![](https://i.imgur.com/4n2NBtp.png)

以上面兩張圖(block size = 16)為例，GOPS = 444461141 * 1246 / 0.38111 / 1024 / 1024 / 1024 ~= 1353.3235。

這幾個metrics都是取phase3的結果來比較(3個phase的結果放在report最後面的附件供參考)，而沒有跑更大一點的測資是因為block size = 8時，效能不是太好，會超過TIME LIMIT，導致hades會砍掉超時的nvprof，因此只有跑c21.1做代表。

|Blocking factor|gld throughput (GB/s)|gst throughput (GB/s)|shared load throughtput (GB/s)|shared store throughput (GB/s)|    GOPS   |
|:--------:|:------------:|:------------:|:---------------------:|:---------------------:|:---------:|
|     8    |   19.05      |   109.8      |       878.39          |        219.69         |  740.0995 |
|    16    |  230.08      |  153.28      |       1840.6          |        306.77         | 1353.3235 |
|    32    |  219.64      |  73.215      |       3514.3          |        292.86         | 1985.4961 |
|    64    |  141.77      |  47.257      |       3024.5          |        94.514         | 2431.1053 |

![](https://i.imgur.com/yEjGBgQ.png)

![](https://i.imgur.com/WALvQGx.png)

![](https://i.imgur.com/87TkuPU.png)

可發現global memory的performance在blocking factor = 16時是最好的，而shared memory的performance在blocking factor = 32時是大約最好的，而computation performance則是在blocking factor = 64時是最好的。


### Optimization

The results below is based on running testcase p12k1.

1. GPU baseline:
    將一開始的seq.cc改成可以在GPU跑。
2. Coalesced memory access:
    修改code，使讀取global memory時是Coalesced地讀取。
3. Unroll:
    使用`#pragma unroll`去unroll迴圈。
4. Shared memory:
    將三個phase分開寫function做處理，根據不同的phase索取不同的shared memory大小，並將重複利用到的global memory load進shared memory中。
5. Modify if-branch to min:
    把使用if判斷兩個值的大小去決定是否要修改最小值，改成取兩者之間的min。如下方程式碼所示。這樣的話可以減少diversity，讓wrap中的所有人都做同樣的事。
```cpp
int diff = k_max - k_min; 
int dist_i_j = Dist_ij[threadIdx.y][threadIdx.x];

/* original version */
#pragma unroll
for (int k = 0; k < diff; ++k){
    int val = Dist_ik[threadIdx.y][k] + Dist_kj[k][threadIdx.x];
    dist_i_j = val * (val < dist_i_j) + dist_i_j * (val >= dist_i_j);
}

/* optimized version */
#pragma unroll
for (int k = 0; k < diff; ++k){
    int val = Dist_ik[threadIdx.y][k]+Dist_kj[k][threadIdx.x];
    dist_i_j = min(val, dist_i_j);
}
```
6. Large blocking factor:
    原先的block size是取32，將block size擴大成64，而thread大小維持(32, 32)不變，只是改成要做4遍的工作。
7. Occupancy optimization:
    在Makefile中調整-maxrregcount="22"。

|   Optimization method   |time (second)|
|:-----------------------:|:----------:|
|      GPU baseline       | 273.616392 |
| Coalesced memory access |  27.610206 |
|        Unroll           |  23.837973 |
|      Shared memory      |   7.482059 |
| Modify if-branch to min |   5.238779 |
|  Large blocking factor  |   2.582597 |
| Occupancy optimization  |   2.177445 |

![](https://i.imgur.com/WHQefqJ.png)



## Experience & conclusion

### What have you learned from this homework?

這次是除了lab以外，第一次寫cuda的程式，發現cuda真的有很多深奧的優化技巧，上課的時候聽老師講說不同的寫法，效能甚至可以差到幾十倍，那時候還覺得有點浮誇，真的會差那麼多嗎？直到自己實際跑了之後才發現光有沒有做coalesced memory access，就可以差到好幾倍了。

中間一度卡在block size = 32，那時2D API、Padding、Coalesced memory、Shared memory、Unroll都做了，卻一直卡在p24k1，直到後來聽到有人說將block size改成64，一次做4遍，會使效能變好，才又抱著希望去嘗試看看，幸好到最後還有繼續堅持寫下去(但大部分是抱持著這次沒有寫出來的話，hw4-2不知道會不會悲劇，所以拼命也要寫過XD)，才能打破很多以前的想法，學到新的東西。

就像老師說的，cuda的程式碼真的會隨著優化而使得可讀性變差，就算是前一天才打完的程式碼，過了一天去看還是要想一下自己到底在寫什麼，真的要保持思緒清楚才不會卡在奇怪的地方。

## 附件

### Global and Shared Memory Performance

* blocking factor = 8
    ![](https://i.imgur.com/OeibBIU.png)
* blocking factor = 16
    ![](https://i.imgur.com/GhvBbtc.png)
* blocking factor = 32
    ![](https://i.imgur.com/d2n5g34.png)
* blocking factor = 64
    ![](https://i.imgur.com/qEzeu1v.png)

